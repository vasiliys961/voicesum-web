# app.py - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º AssemblyAI
from flask import Flask, render_template, request, jsonify
import os
import tempfile
import assemblyai as aai
from openai import OpenAI
import time
import logging
import signal
import sys

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder="static", template_folder="templates")
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500 MB

# === API –∫–ª—é—á–∏ ===
ASSEMBLYAI_API_KEY = os.environ.get("ASSEMBLYAI_API_KEY", "fb277d535ab94838bc14cc2f687b30be")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
TEMP_DIR = tempfile.mkdtemp(prefix="voicesum_hybrid_")

logger.info(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞: {TEMP_DIR}")

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ AssemblyAI ===
aai.settings.api_key = ASSEMBLYAI_API_KEY
logger.info("üîë AssemblyAI API –∫–ª—é—á –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ ===
def get_transcription_config_auto():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º —è–∑—ã–∫–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
    return aai.TranscriptionConfig(
        # === –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
        speech_model=aai.SpeechModel.best,  # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        # language_code –ù–ï —É–∫–∞–∑—ã–≤–∞–µ–º - –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ
        
        # === AI —Ñ—É–Ω–∫—Ü–∏–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±—Ä–∞–Ω –∫–æ–Ω—Ñ–ª–∏–∫—Ç) ===
        speaker_labels=True,  # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤
        speakers_expected=2,  # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        auto_highlights=True,  # –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í—ã–±–∏—Ä–∞–µ–º –õ–ò–ë–û auto_chapters, –õ–ò–ë–û summarization
        summarization=True,       # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ
        summary_model=aai.SummarizationModel.informative,
        summary_type=aai.SummarizationType.bullets,
        # auto_chapters=True,   # –û–¢–ö–õ–Æ–ß–ï–ù–û –∏–∑-–∑–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ —Å summarization
        
        sentiment_analysis=True,  # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π
        entity_detection=True,    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        
        content_safety=True,      # –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        iab_categories=True,      # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Ç–µ–º–∞–º
        
        # === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ===
        punctuate=True,          # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        format_text=True,        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        dual_channel=False,      # –ú–æ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
        
        # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        disfluencies=False,      # –£–±–∏—Ä–∞–µ–º "—ç–º", "–∞—Ö"
        filter_profanity=False,  # –ù–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –º–∞—Ç
        redact_pii=False,        # –ù–µ —Å–∫—Ä—ã–≤–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    )

def get_transcription_config_auto_chapters():
    """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –≥–ª–∞–≤–∞–º–∏ –≤–º–µ—Å—Ç–æ —Ä–µ–∑—é–º–µ"""
    return aai.TranscriptionConfig(
        # === –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
        speech_model=aai.SpeechModel.best,
        
        # === AI —Ñ—É–Ω–∫—Ü–∏–∏ —Å –≥–ª–∞–≤–∞–º–∏ ===
        speaker_labels=True,
        speakers_expected=2,
        
        auto_highlights=True,
        auto_chapters=True,       # –ì–ª–∞–≤—ã –≤–º–µ—Å—Ç–æ —Ä–µ–∑—é–º–µ
        # summarization –ù–ï –≤–∫–ª—é—á–∞–µ–º
        
        sentiment_analysis=True,
        entity_detection=True,
        content_safety=True,
        iab_categories=True,
        
        # === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ===
        punctuate=True,
        format_text=True,
        dual_channel=False,
        
        # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        disfluencies=False,
        filter_profanity=False,
        redact_pii=False,
    )

def get_transcription_config_russian():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    return aai.TranscriptionConfig(
        # === –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
        speech_model=aai.SpeechModel.best,  # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        language_code="ru",  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
        
        # === –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        speaker_labels=True,  # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤ ‚úÖ
        speakers_expected=2,  # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        entity_detection=True,    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π ‚úÖ
        
        # === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ===
        punctuate=True,          # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        format_text=True,        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        dual_channel=False,      # –ú–æ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
        
        # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        disfluencies=False,      # –£–±–∏—Ä–∞–µ–º "—ç–º", "–∞—Ö"
        filter_profanity=False,  # –ù–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –º–∞—Ç
        redact_pii=False,        # –ù–µ —Å–∫—Ä—ã–≤–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    )

def transcribe_with_fallback(file_path):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º fallback"""
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å —Ä–µ–∑—é–º–µ
    try:
        logger.info("üåç –ü—Ä–æ–±—É—é –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Å —Ä–µ–∑—é–º–µ...")
        config_auto = get_transcription_config_auto()
        transcriber = aai.Transcriber(config=config_auto)
        transcript = transcriber.transcribe(file_path)
        
        if transcript.status == aai.TranscriptStatus.error:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {transcript.error}")
        
        logger.info("‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º (—Ä–µ–∑—é–º–µ) —É—Å–ø–µ—à–Ω–∞!")
        return transcript, "auto_detection_summary"
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å —Ä–µ–∑—é–º–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å –≥–ª–∞–≤–∞–º–∏
        try:
            logger.info("üåç –ü—Ä–æ–±—É—é –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Å –≥–ª–∞–≤–∞–º–∏...")
            config_chapters = get_transcription_config_auto_chapters()
            transcriber = aai.Transcriber(config=config_chapters)
            transcript = transcriber.transcribe(file_path)
            
            if transcript.status == aai.TranscriptStatus.error:
                raise RuntimeError(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å –≥–ª–∞–≤–∞–º–∏: {transcript.error}")
            
            logger.info("‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º (–≥–ª–∞–≤—ã) —É—Å–ø–µ—à–Ω–∞!")
            return transcript, "auto_detection_chapters"
            
        except Exception as e2:
            logger.warning(f"‚ö†Ô∏è –ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å –≥–ª–∞–≤–∞–º–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ: {e2}")
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ (fallback)
            try:
                logger.info("üá∑üá∫ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏)...")
                config_ru = get_transcription_config_russian()
                transcriber = aai.Transcriber(config=config_ru)
                transcript = transcriber.transcribe(file_path)
                
                if transcript.status == aai.TranscriptStatus.error:
                    raise RuntimeError(f"–û—à–∏–±–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ: {transcript.error}")
                
                logger.info("‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —É—Å–ø–µ—à–Ω–∞!")
                return transcript, "russian_limited_features"
                
            except Exception as e3:
                logger.error(f"‚ùå –í—Å–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏: –∞–≤—Ç–æ-—Ä–µ–∑—é–º–µ({e}), –∞–≤—Ç–æ-–≥–ª–∞–≤—ã({e2}), —Ä—É—Å—Å–∫–∏–π({e3})")
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª –≤—Å–µ–º–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏")

# === –£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ ===
class AdvancedSummarizer:
    def __init__(self, openrouter_key):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=openrouter_key
        ) if openrouter_key else None
    
    def create_smart_summary(self, transcript_result, transcription_method):
        """–°–æ–∑–¥–∞–µ—Ç —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö AssemblyAI"""
        if not self.client:
            return self._create_basic_summary(transcript_result, transcription_method)
        
        try:
            # –¢–∞–π–º–∞—É—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ
            timeout_seconds = 60
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            transcript = transcript_result.text
            chapters = getattr(transcript_result, 'chapters', None) or []
            highlights = getattr(transcript_result, 'auto_highlights', None)
            sentiment = getattr(transcript_result, 'sentiment_analysis_results', None) or []
            entities = getattr(transcript_result, 'entities', None) or []
            builtin_summary = getattr(transcript_result, 'summary', None)
            detected_language = getattr(transcript_result, 'language_code', 'unknown')
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –±–æ–≥–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä)
            context = f"–ü–û–õ–ù–´–ô –¢–†–ê–ù–°–ö–†–ò–ü–¢:\n{transcript[:20000]}\n\n"
            context += f"–ú–ï–¢–û–î –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–ò: {transcription_method}\n"
            context += f"–û–ü–†–ï–î–ï–õ–ï–ù–ù–´–ô –Ø–ó–´–ö: {detected_language}\n\n"
            
            if chapters:
                context += "üìö –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ì–õ–ê–í–´:\n"
                for i, chapter in enumerate(chapters[:8], 1):
                    headline = getattr(chapter, 'headline', f'–ì–ª–∞–≤–∞ {i}')
                    start_time = getattr(chapter, 'start', 0) / 1000 / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö
                    context += f"{i}. {headline} ({start_time:.1f}–º–∏–Ω)\n"
                context += "\n"
            
            if highlights and hasattr(highlights, 'results'):
                context += "üí° –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:\n"
                for highlight in highlights.results[:10]:
                    text = getattr(highlight, 'text', '')
                    if text:
                        context += f"‚Ä¢ {text}\n"
                context += "\n"
            
            if entities:
                context += "üè∑Ô∏è –£–ü–û–ú–Ø–ù–£–¢–´–ï –°–£–©–ù–û–°–¢–ò:\n"
                entity_groups = {}
                for entity in entities[:20]:
                    entity_type = getattr(entity, 'entity_type', 'other')
                    entity_text = getattr(entity, 'text', '')
                    if entity_type not in entity_groups:
                        entity_groups[entity_type] = []
                    if entity_text not in entity_groups[entity_type]:
                        entity_groups[entity_type].append(entity_text)
                
                for entity_type, texts in entity_groups.items():
                    context += f"  {entity_type}: {', '.join(texts[:3])}\n"
                context += "\n"
            
            if builtin_summary:
                context += f"ü§ñ –ë–ê–ó–û–í–û–ï –†–ï–ó–Æ–ú–ï ASSEMBLYAI:\n{builtin_summary}\n\n"
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–∞
            content_language = detected_language if detected_language != 'unknown' else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'
            
            system_prompt = f"""–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∞—É–¥–∏–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞. 
–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω –Ω–∞ —è–∑—ã–∫–µ: {content_language}. –ú–µ—Ç–æ–¥: {transcription_method}.

–í–ê–ñ–ù–û: –°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –°–¢–†–û–ì–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï.

–°–¢–†–£–ö–¢–£–†–ê –†–ï–ó–Æ–ú–ï:
üìã –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
üåç –Ø–ó–´–ö –ö–û–ù–¢–ï–ù–¢–ê: {content_language}
üéØ –û–°–ù–û–í–ù–´–ï –¢–ï–ú–´ (—Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –µ—Å–ª–∏ –µ—Å—Ç—å)
üë• –£–ß–ê–°–¢–ù–ò–ö–ò (–µ—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Å–ø–∏–∫–µ—Ä—ã)
üí° –ö–õ–Æ–ß–ï–í–´–ï –ò–ù–°–ê–ô–¢–´
üìä –í–ê–ñ–ù–´–ï –§–ê–ö–¢–´ –ò –¶–ò–§–†–´
üé≠ –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨
‚úÖ –†–ï–®–ï–ù–ò–Ø –ò –î–ï–ô–°–¢–í–ò–Ø
üè∑Ô∏è –ö–õ–Æ–ß–ï–í–´–ï –ü–ï–†–°–û–ù–´

–ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏, –±—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –ø–µ—Ä–µ–≤–æ–¥–∏ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫."""
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            response = self.client.chat.completions.create(
                model="anthropic/claude-3-haiku",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": context}
                ],
                max_tokens=1200,
                temperature=0.3,
                timeout=timeout_seconds
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–º–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
            return self._create_basic_summary(transcript_result, transcription_method)
    
    def _create_basic_summary(self, transcript_result, transcription_method):
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –∏–∑ –¥–∞–Ω–Ω—ã—Ö AssemblyAI –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï"""
        summary_parts = []
        detected_language = getattr(transcript_result, 'language_code', 'unknown')
        
        language_names = {
            'en': '–∞–Ω–≥–ª–∏–π—Å–∫–∏–π', 'ru': '—Ä—É—Å—Å–∫–∏–π', 'es': '–∏—Å–ø–∞–Ω—Å–∫–∏–π',
            'fr': '—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π', 'de': '–Ω–µ–º–µ—Ü–∫–∏–π', 'it': '–∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π',
            'pt': '–ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π', 'zh': '–∫–∏—Ç–∞–π—Å–∫–∏–π', 'ja': '—è–ø–æ–Ω—Å–∫–∏–π',
            'ko': '–∫–æ—Ä–µ–π—Å–∫–∏–π', 'unknown': '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'
        }
        
        language_display = language_names.get(detected_language, detected_language)
        
        summary_parts.append(f"üîß –ú–ï–¢–û–î –û–ë–†–ê–ë–û–¢–ö–ò: {transcription_method}")
        summary_parts.append(f"üåç –Ø–ó–´–ö –ö–û–ù–¢–ï–ù–¢–ê: {language_display}")
        
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ
        builtin_summary = getattr(transcript_result, 'summary', None)
        if builtin_summary:
            summary_parts.append(f"\nüìã –ë–ê–ó–û–í–û–ï –†–ï–ó–Æ–ú–ï:\n{builtin_summary}")
        
        # –ì–ª–∞–≤—ã
        chapters = getattr(transcript_result, 'chapters', None)
        if chapters:
            summary_parts.append("\nüéØ –û–°–ù–û–í–ù–´–ï –†–ê–ó–î–ï–õ–´:")
            for i, chapter in enumerate(chapters[:6], 1):
                headline = getattr(chapter, 'headline', f'–†–∞–∑–¥–µ–ª {i}')
                start_time = getattr(chapter, 'start', 0) / 1000 / 60
                summary_parts.append(f"{i}. {headline} ({start_time:.1f}–º–∏–Ω)")
        
        # –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
        highlights = getattr(transcript_result, 'auto_highlights', None)
        if highlights and hasattr(highlights, 'results'):
            summary_parts.append(f"\nüí° –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:")
            for highlight in highlights.results[:8]:
                text = getattr(highlight, 'text', '').strip()
                if text:
                    summary_parts.append(f"‚Ä¢ {text}")
        
        # –°—É—â–Ω–æ—Å—Ç–∏
        entities = getattr(transcript_result, 'entities', None)
        if entities:
            summary_parts.append("\nüè∑Ô∏è –£–ü–û–ú–Ø–ù–£–¢–´–ï –°–£–©–ù–û–°–¢–ò:")
            entity_groups = {}
            
            entity_type_translation = {
                'person': '–õ—é–¥–∏', 'organization': '–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏', 
                'location': '–ú–µ—Å—Ç–∞', 'date': '–î–∞—Ç—ã', 'money': '–î–µ–Ω—å–≥–∏',
                'phone_number': '–¢–µ–ª–µ—Ñ–æ–Ω—ã', 'email': 'Email',
                'product': '–ü—Ä–æ–¥—É–∫—Ç—ã', 'event': '–°–æ–±—ã—Ç–∏—è', 'other': '–ü—Ä–æ—á–µ–µ'
            }
            
            for entity in entities[:12]:
                entity_type = getattr(entity, 'entity_type', 'other')
                entity_text = getattr(entity, 'text', '')
                translated_type = entity_type_translation.get(entity_type, entity_type)
                
                if translated_type not in entity_groups:
                    entity_groups[translated_type] = []
                if entity_text not in entity_groups[translated_type]:
                    entity_groups[translated_type].append(entity_text)
            
            for entity_type, texts in entity_groups.items():
                if texts:
                    summary_parts.append(f"  {entity_type}: {', '.join(texts[:3])}")
        
        if not summary_parts:
            summary_parts.append("üìã –ë–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
        
        return "\n".join(summary_parts)

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
summarizer = AdvancedSummarizer(OPENROUTER_API_KEY)

# === –£–ª—É—á—à–µ–Ω–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
def analyze_sentiment_overall(sentiment_results):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—â—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"""
    if not sentiment_results:
        return "not_analyzed", 0, 0, 0
    
    positive_count = sum(1 for s in sentiment_results if getattr(s, 'sentiment', '') == 'POSITIVE')
    negative_count = sum(1 for s in sentiment_results if getattr(s, 'sentiment', '') == 'NEGATIVE')
    neutral_count = len(sentiment_results) - positive_count - negative_count
    
    if positive_count > negative_count and positive_count > neutral_count:
        return "positive", positive_count, negative_count, neutral_count
    elif negative_count > positive_count and negative_count > neutral_count:
        return "negative", positive_count, negative_count, neutral_count
    else:
        return "neutral", positive_count, negative_count, neutral_count

def format_entities_by_type(entities):
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º"""
    if not entities:
        return {}
    
    entity_groups = {}
    for entity in entities:
        entity_type = getattr(entity, 'entity_type', 'other')
        entity_text = getattr(entity, 'text', '')
        
        if entity_type not in entity_groups:
            entity_groups[entity_type] = []
        
        if entity_text and entity_text not in entity_groups[entity_type]:
            entity_groups[entity_type].append(entity_text)
    
    return entity_groups

def get_transcription_features(transcription_method):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞"""
    if transcription_method == "auto_detection_summary":
        return [
            "üéôÔ∏è –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏",
            "üåç –ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —è–∑—ã–∫–∞",
            "üë• –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤", 
            "üí° –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã",
            "üé≠ –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
            "üè∑Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π",
            "ü§ñ –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ",
            "üß† –£–º–Ω–æ–µ —Ä–µ–∑—é–º–µ Claude",
            "üõ°Ô∏è –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º"
        ]
    elif transcription_method == "auto_detection_chapters":
        return [
            "üéôÔ∏è –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏",
            "üåç –ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —è–∑—ã–∫–∞",
            "üë• –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤", 
            "üìö –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥–ª–∞–≤—ã",
            "üí° –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã",
            "üé≠ –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
            "üè∑Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π",
            "üß† –£–º–Ω–æ–µ —Ä–µ–∑—é–º–µ Claude",
            "üõ°Ô∏è –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º"
        ]
    else:
        return [
            "üéôÔ∏è –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏",
            "üá∑üá∫ –†—É—Å—Å–∫–∏–π —è–∑—ã–∫",
            "üë• –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤",
            "üè∑Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π",
            "üß† –£–º–Ω–æ–µ —Ä–µ–∑—é–º–µ Claude"
        ]

# === –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ ===
def signal_handler(signum, frame):
    logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è, –æ—á–∏—â–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã...")
    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    import shutil
    try:
        if os.path.exists(TEMP_DIR):
            shutil.rmtree(TEMP_DIR)
    except:
        pass
    sys.exit(0)

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

# === –ú–∞—Ä—à—Ä—É—Ç—ã ===

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/health")
def health():
    return jsonify({
        "status": "ok",
        "service": "AssemblyAI Hybrid Approach (Fixed)",
        "api_configured": bool(ASSEMBLYAI_API_KEY),
        "openrouter_configured": bool(OPENROUTER_API_KEY),
        "transcription_strategies": [
            "1. Auto-detection with summary",
            "2. Auto-detection with chapters", 
            "3. Russian language (fallback)"
        ],
        "fixes_applied": [
            "‚úÖ –£–±—Ä–∞–Ω –∫–æ–Ω—Ñ–ª–∏–∫—Ç auto_chapters + summarization",
            "‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è fallback",
            "‚úÖ –£–ª—É—á—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫",
            "‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã —Ç–∞–π–º–∞—É—Ç—ã",
            "‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
        ],
        "limits": {
            "free_credits": "$50 (185 hours)",
            "max_file_size": "500MB",
            "max_duration": "unlimited",
            "parallel_processing": "5 files"
        }
    })

@app.route("/transcribe", methods=["POST"])
def transcribe():
    start_time = time.time()
    input_path = None
    
    try:
        if 'audio' not in request.files:
            return jsonify({"error": "–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω"}), 400

        file = request.files['audio']
        if file.filename == '':
            return jsonify({"error": "–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω"}), 400

        timestamp = int(time.time() * 1000)
        input_path = os.path.join(TEMP_DIR, f"hybrid_{timestamp}.{file.filename.split('.')[-1]}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        file.save(input_path)
        file_size = os.path.getsize(input_path)
        logger.info(f"üì• –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {file_size / 1024 / 1024:.1f} MB")
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        estimated_duration = file_size / 1024 / 1024  # –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ –≤ –º–∏–Ω—É—Ç–∞—Ö
        logger.info(f"üìä –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: ~{estimated_duration:.1f} –º–∏–Ω—É—Ç")
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º fallback
        transcript, transcription_method = transcribe_with_fallback(input_path)
        
        if not transcript.text:
            return jsonify({"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é"}), 400
        
        logger.info(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–º: {transcription_method}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ—á–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ AssemblyAI
        audio_duration_ms = getattr(transcript, 'audio_duration', None)
        actual_duration = audio_duration_ms / 1000 / 60 if audio_duration_ms else estimated_duration
        detected_language = getattr(transcript, 'language_code', 'unknown')
        
        # –°–æ–∑–¥–∞–µ–º —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            logger.info("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É—é —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ...")
            summary = summarizer.create_smart_summary(transcript, transcription_method)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–º–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
            summary = summarizer._create_basic_summary(transcript, transcription_method)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞
        sentiment_analysis = getattr(transcript, 'sentiment_analysis_results', None) or []
        overall_sentiment, pos_count, neg_count, neu_count = analyze_sentiment_overall(sentiment_analysis)
        
        entities = getattr(transcript, 'entities', None) or []
        entities_by_type = format_entities_by_type(entities)
        
        chapters = getattr(transcript, 'chapters', None) or []
        highlights = getattr(transcript, 'auto_highlights', None)
        
        # –ü–æ–¥—Å—á–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫—Ä–µ–¥–∏—Ç–æ–≤
        credits_used = actual_duration / 60 * 0.37  # –ø—Ä–∏–º–µ—Ä–Ω–æ $0.37 –∑–∞ —á–∞—Å
        
        total_time = time.time() - start_time
        logger.info(f"‚úÖ –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.1f}—Å")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        response_data = {
            "transcript": transcript.text,
            "summary": summary,
            "service_used": "AssemblyAI Hybrid Approach (Fixed)",
            "transcription_method": transcription_method,
            "detected_language": detected_language,
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            "statistics": {
                "processing_time": f"{total_time:.1f}s",
                "audio_duration": f"{actual_duration:.1f}min", 
                "file_size": f"{file_size / 1024 / 1024:.1f}MB",
                "confidence": getattr(transcript, 'confidence', 0),
                "credits_used": f"${credits_used:.3f}",
                "words_count": len(transcript.text.split()) if transcript.text else 0
            },
            
            # AI –∞–Ω–∞–ª–∏–∑
            "ai_analysis": {
                "method_used": transcription_method,
                "speakers_detected": len(set([utterance.speaker for utterance in getattr(transcript, 'utterances', []) if utterance.speaker])),
                "chapters_found": len(chapters),
                "highlights_found": len(highlights.results) if highlights and hasattr(highlights, 'results') else 0,
                "entities_found": len(entities),
                "sentiment_breakdown": {
                    "overall": overall_sentiment,
                    "positive_segments": pos_count,
                    "negative_segments": neg_count,
                    "neutral_segments": neu_count
                },
                "features_available": get_transcription_features(transcription_method)
            }
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if chapters:
            response_data["chapters"] = [
                {
                    "headline": getattr(ch, 'headline', ''),
                    "start_time": f"{getattr(ch, 'start', 0)/1000/60:.1f}min",
                    "end_time": f"{getattr(ch, 'end', 0)/1000/60:.1f}min",
                    "summary": getattr(ch, 'summary', '')
                }
                for ch in chapters[:12]
            ]
        
        if highlights and hasattr(highlights, 'results'):
            response_data["key_highlights"] = [
                {
                    "text": getattr(h, 'text', ''),
                    "rank": getattr(h, 'rank', 0),
                    "start_time": f"{getattr(h, 'start', 0)/1000/60:.1f}min"
                }
                for h in highlights.results[:15]
            ]
        
        if entities_by_type:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–Ω–æ—Å—Ç–µ–π –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
            limited_entities = {}
            for entity_type, entity_list in entities_by_type.items():
                limited_entities[entity_type] = entity_list[:8]
            response_data["entities_by_type"] = limited_entities
        
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –æ—Ç AssemblyAI (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        builtin_summary = getattr(transcript, 'summary', None)
        if builtin_summary:
            response_data["assemblyai_summary"] = builtin_summary
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–µ—Ç–æ–¥–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        method_names = {
            "auto_detection_summary": "–ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Å —Ä–µ–∑—é–º–µ",
            "auto_detection_chapters": "–ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Å –≥–ª–∞–≤–∞–º–∏", 
            "russian_limited_features": "–†—É—Å—Å–∫–∏–π —è–∑—ã–∫ (fallback)"
        }
        
        response_data["method_info"] = {
            "name": method_names.get(transcription_method, transcription_method),
            "features": "–í—Å–µ AI —Ñ—É–Ω–∫—Ü–∏–∏" if "auto_detection" in transcription_method else "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä",
            "language_detected": detected_language,
            "fallback_used": transcription_method == "russian_limited_features"
        }

        return jsonify(response_data)
