# app.py - –í–µ—Ä—Å–∏—è –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç ffmpeg
from flask import Flask, render_template, request, jsonify
import os
import tempfile
import assemblyai as aai
from openai import OpenAI
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder="static", template_folder="templates")
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500 MB

# === API –∫–ª—é—á–∏ ===
ASSEMBLYAI_API_KEY = os.environ.get("ASSEMBLYAI_API_KEY", "fb277d535ab94838bc14cc2f687b30be")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
TEMP_DIR = tempfile.mkdtemp(prefix="voicesum_aai_")

logger.info(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞: {TEMP_DIR}")

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ AssemblyAI ===
aai.settings.api_key = ASSEMBLYAI_API_KEY
logger.info("üîë AssemblyAI API –∫–ª—é—á –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ ===
def get_transcription_config():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏"""
    return aai.TranscriptionConfig(
        # === –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
        speech_model=aai.SpeechModel.best,  # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        language_code="ru",  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
        
        # === AI —Ñ—É–Ω–∫—Ü–∏–∏ (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ!) ===
        speaker_labels=True,  # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤
        speakers_expected=2,  # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        auto_highlights=True,  # –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
        auto_chapters=True,   # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥–ª–∞–≤—ã
        
        sentiment_analysis=True,  # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π
        entity_detection=True,    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        
        content_safety=True,      # –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        iab_categories=True,      # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Ç–µ–º–∞–º
        
        summarization=True,       # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ
        summary_model=aai.SummarizationModel.informative,
        summary_type=aai.SummarizationType.bullets,
        
        # === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ===
        punctuate=True,          # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        format_text=True,        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        dual_channel=False,      # –ú–æ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
        
        # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        disfluencies=False,      # –£–±–∏—Ä–∞–µ–º "—ç–º", "–∞—Ö"
        filter_profanity=False,  # –ù–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –º–∞—Ç
        redact_pii=False,        # –ù–µ —Å–∫—Ä—ã–≤–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    )

# === –£–º–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ ===
class AdvancedSummarizer:
    def __init__(self, openrouter_key):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=openrouter_key
        ) if openrouter_key else None
    
    def create_smart_summary(self, transcript_result):
        """–°–æ–∑–¥–∞–µ—Ç —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö AssemblyAI"""
        if not self.client:
            return self._create_basic_summary(transcript_result)
        
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            transcript = transcript_result.text
            chapters = getattr(transcript_result, 'chapters', None) or []
            highlights = getattr(transcript_result, 'auto_highlights', None)
            sentiment = getattr(transcript_result, 'sentiment_analysis_results', None) or []
            entities = getattr(transcript_result, 'entities', None) or []
            builtin_summary = getattr(transcript_result, 'summary', None)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –±–æ–≥–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = f"–ü–û–õ–ù–´–ô –¢–†–ê–ù–°–ö–†–ò–ü–¢:\n{transcript[:25000]}\n\n"
            
            if chapters:
                context += "üìö –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ì–õ–ê–í–´:\n"
                for i, chapter in enumerate(chapters[:10], 1):
                    headline = getattr(chapter, 'headline', f'–ì–ª–∞–≤–∞ {i}')
                    start_time = getattr(chapter, 'start', 0) / 1000 / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö
                    context += f"{i}. {headline} ({start_time:.1f}–º–∏–Ω)\n"
                context += "\n"
            
            if highlights and hasattr(highlights, 'results'):
                context += "üí° –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:\n"
                for highlight in highlights.results[:15]:
                    text = getattr(highlight, 'text', '')
                    rank = getattr(highlight, 'rank', 0)
                    if text:
                        context += f"‚Ä¢ {text} (–≤–∞–∂–Ω–æ—Å—Ç—å: {rank:.2f})\n"
                context += "\n"
            
            if entities:
                context += "üè∑Ô∏è –£–ü–û–ú–Ø–ù–£–¢–´–ï –°–£–©–ù–û–°–¢–ò:\n"
                entity_groups = {}
                for entity in entities[:25]:
                    entity_type = getattr(entity, 'entity_type', 'other')
                    entity_text = getattr(entity, 'text', '')
                    if entity_type not in entity_groups:
                        entity_groups[entity_type] = []
                    if entity_text not in entity_groups[entity_type]:
                        entity_groups[entity_type].append(entity_text)
                
                for entity_type, texts in entity_groups.items():
                    context += f"  {entity_type}: {', '.join(texts[:5])}\n"
                context += "\n"
            
            if builtin_summary:
                context += f"ü§ñ –ë–ê–ó–û–í–û–ï –†–ï–ó–Æ–ú–ï ASSEMBLYAI:\n{builtin_summary}\n\n"
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            if sentiment:
                positive_count = sum(1 for s in sentiment if getattr(s, 'sentiment', '') == 'POSITIVE')
                negative_count = sum(1 for s in sentiment if getattr(s, 'sentiment', '') == 'NEGATIVE')
                total_sentiment = len(sentiment)
                if total_sentiment > 0:
                    context += f"üé≠ –û–ë–©–ê–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨: {positive_count}/{total_sentiment} –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö, {negative_count}/{total_sentiment} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö\n\n"
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ
            response = self.client.chat.completions.create(
                model="anthropic/claude-3-haiku",
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∞—É–¥–∏–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã —Å –¥–µ–ª–æ–≤—ã–º–∏ –≤—Å—Ç—Ä–µ—á–∞–º–∏, "
                            "–∏–Ω—Ç–µ—Ä–≤—å—é, –ª–µ–∫—Ü–∏—è–º–∏ –∏ –ø–æ–¥–∫–∞—Å—Ç–∞–º–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –±–æ–≥–∞—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ–∑–¥–∞–π "
                            "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n\n"
                            
                            "–°–¢–†–£–ö–¢–£–†–ê –†–ï–ó–Æ–ú–ï:\n"
                            "üìã –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - —Å—É—Ç—å –∑–∞–ø–∏—Å–∏)\n"
                            "üéØ –û–°–ù–û–í–ù–´–ï –¢–ï–ú–´ –ò –†–ê–ó–î–ï–õ–´ (—Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –µ—Å–ª–∏ –µ—Å—Ç—å)\n"
                            "üë• –£–ß–ê–°–¢–ù–ò–ö–ò –ò –†–û–õ–ò (–µ—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Å–ø–∏–∫–µ—Ä—ã)\n"
                            "üí° –ö–õ–Æ–ß–ï–í–´–ï –ò–ù–°–ê–ô–¢–´ –ò –í–´–í–û–î–´\n"
                            "üìä –í–ê–ñ–ù–´–ï –§–ê–ö–¢–´, –¶–ò–§–†–´, –î–ê–¢–´\n"
                            "üé≠ –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨\n"
                            "‚úÖ –†–ï–®–ï–ù–ò–Ø, –î–ï–ô–°–¢–í–ò–Ø, NEXT STEPS\n"
                            "üè∑Ô∏è –ö–õ–Æ–ß–ï–í–´–ï –ü–ï–†–°–û–ù–´ –ò –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò\n\n"
                            
                            "–¢–†–ï–ë–û–í–ê–ù–ò–Ø:\n"
                            "- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n"
                            "- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º\n"
                            "- –í—ã–¥–µ–ª–∏ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ\n"
                            "- –°–æ—Ö—Ä–∞–Ω—è–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω\n"
                            "- –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n"
                            "- –ò—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –æ—Ç AssemblyAI –∫–∞–∫ –æ—Å–Ω–æ–≤—É"
                        )
                    },
                    {"role": "user", "content": context}
                ],
                max_tokens=1500,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–º–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
            return self._create_basic_summary(transcript_result)
    
    def _create_basic_summary(self, transcript_result):
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –∏–∑ –¥–∞–Ω–Ω—ã—Ö AssemblyAI"""
        summary_parts = []
        
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ
        builtin_summary = getattr(transcript_result, 'summary', None)
        if builtin_summary:
            summary_parts.append(f"üìã –†–ï–ó–Æ–ú–ï:\n{builtin_summary}")
        
        # –ì–ª–∞–≤—ã
        chapters = getattr(transcript_result, 'chapters', None)
        if chapters:
            summary_parts.append("\nüéØ –û–°–ù–û–í–ù–´–ï –†–ê–ó–î–ï–õ–´:")
            for i, chapter in enumerate(chapters[:8], 1):
                headline = getattr(chapter, 'headline', f'–†–∞–∑–¥–µ–ª {i}')
                start_time = getattr(chapter, 'start', 0) / 1000 / 60
                summary_parts.append(f"{i}. {headline} ({start_time:.1f}–º–∏–Ω)")
        
        # –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
        highlights = getattr(transcript_result, 'auto_highlights', None)
        if highlights and hasattr(highlights, 'results'):
            summary_parts.append("\nüí° –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:")
            for highlight in highlights.results[:10]:
                text = getattr(highlight, 'text', '').strip()
                if text:
                    summary_parts.append(f"‚Ä¢ {text}")
        
        return "\n".join(summary_parts) if summary_parts else "üìã –ë–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
summarizer = AdvancedSummarizer(OPENROUTER_API_KEY)

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
def analyze_sentiment_overall(sentiment_results):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—â—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"""
    if not sentiment_results:
        return "neutral", 0, 0, 0
    
    positive_count = sum(1 for s in sentiment_results if getattr(s, 'sentiment', '') == 'POSITIVE')
    negative_count = sum(1 for s in sentiment_results if getattr(s, 'sentiment', '') == 'NEGATIVE')
    neutral_count = len(sentiment_results) - positive_count - negative_count
    
    if positive_count > negative_count and positive_count > neutral_count:
        return "positive", positive_count, negative_count, neutral_count
    elif negative_count > positive_count and negative_count > neutral_count:
        return "negative", positive_count, negative_count, neutral_count
    else:
        return "neutral", positive_count, negative_count, neutral_count

def format_entities_by_type(entities):
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º"""
    if not entities:
        return {}
    
    entity_groups = {}
    for entity in entities:
        entity_type = getattr(entity, 'entity_type', 'other')
        entity_text = getattr(entity, 'text', '')
        
        if entity_type not in entity_groups:
            entity_groups[entity_type] = []
        
        if entity_text and entity_text not in entity_groups[entity_type]:
            entity_groups[entity_type].append(entity_text)
    
    return entity_groups

# === –ú–∞—Ä—à—Ä—É—Ç—ã ===

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/health")
def health():
    return jsonify({
        "status": "ok",
        "service": "AssemblyAI Official SDK (No ffmpeg dependency)",
        "api_configured": bool(ASSEMBLYAI_API_KEY),
        "openrouter_configured": bool(OPENROUTER_API_KEY),
        "features": [
            "üéôÔ∏è –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏",
            "üë• –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤", 
            "üìö –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥–ª–∞–≤—ã",
            "üí° –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã",
            "üé≠ –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
            "üè∑Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π",
            "ü§ñ –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ",
            "üß† –£–º–Ω–æ–µ —Ä–µ–∑—é–º–µ Claude",
            "üõ°Ô∏è –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º"
        ],
        "limits": {
            "free_credits": "$50 (185 hours)",
            "max_file_size": "500MB",
            "max_duration": "unlimited",
            "parallel_processing": "5 files"
        }
    })

@app.route("/transcribe", methods=["POST"])
def transcribe():
    start_time = time.time()
    input_path = None
    
    try:
        if 'audio' not in request.files:
            return jsonify({"error": "–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω"}), 400

        file = request.files['audio']
        if file.filename == '':
            return jsonify({"error": "–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω"}), 400

        timestamp = int(time.time() * 1000)
        input_path = os.path.join(TEMP_DIR, f"aai_{timestamp}.{file.filename.split('.')[-1]}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        file.save(input_path)
        file_size = os.path.getsize(input_path)
        logger.info(f"üì• –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {file_size / 1024 / 1024:.1f} MB")
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–µ–∑ pydub
        # –î–ª—è MP3: ~1MB = ~1 –º–∏–Ω—É—Ç–∞ –ø—Ä–∏ —Å—Ä–µ–¥–Ω–µ–º –∫–∞—á–µ—Å—Ç–≤–µ
        estimated_duration = file_size / 1024 / 1024  # –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ –≤ –º–∏–Ω—É—Ç–∞—Ö
        logger.info(f"üìä –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: ~{estimated_duration:.1f} –º–∏–Ω—É—Ç")
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∞–π–±–µ—Ä —Å –Ω–∞—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        config = get_transcription_config()
        transcriber = aai.Transcriber(config=config)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é AssemblyAI...")
        transcript = transcriber.transcribe(input_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if transcript.status == aai.TranscriptStatus.error:
            error_msg = getattr(transcript, 'error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
            raise RuntimeError(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {error_msg}")
        
        if not transcript.text:
            return jsonify({"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é"}), 400
        
        logger.info("‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ—á–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ AssemblyAI
        audio_duration_ms = getattr(transcript, 'audio_duration', None)
        actual_duration = audio_duration_ms / 1000 / 60 if audio_duration_ms else estimated_duration
        
        # –°–æ–∑–¥–∞–µ–º —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ
        logger.info("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É—é —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ...")
        summary = summarizer.create_smart_summary(transcript)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        sentiment_analysis = getattr(transcript, 'sentiment_analysis_results', None) or []
        overall_sentiment, pos_count, neg_count, neu_count = analyze_sentiment_overall(sentiment_analysis)
        
        entities = getattr(transcript, 'entities', None) or []
        entities_by_type = format_entities_by_type(entities)
        
        chapters = getattr(transcript, 'chapters', None) or []
        highlights = getattr(transcript, 'auto_highlights', None)
        
        # –ü–æ–¥—Å—á–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫—Ä–µ–¥–∏—Ç–æ–≤
        credits_used = actual_duration / 60 * 0.37  # –ø—Ä–∏–º–µ—Ä–Ω–æ $0.37 –∑–∞ —á–∞—Å
        
        total_time = time.time() - start_time
        logger.info(f"‚úÖ –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.1f}—Å")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        response_data = {
            "transcript": transcript.text,
            "summary": summary,
            "service_used": "AssemblyAI Official SDK",
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            "statistics": {
                "processing_time": f"{total_time:.1f}s",
                "audio_duration": f"{actual_duration:.1f}min", 
                "file_size": f"{file_size / 1024 / 1024:.1f}MB",
                "confidence": getattr(transcript, 'confidence', 0),
                "credits_used": f"${credits_used:.3f}",
                "words_count": len(transcript.text.split()) if transcript.text else 0
            },
            
            # AI –∞–Ω–∞–ª–∏–∑
            "ai_analysis": {
                "speakers_detected": len(set([utterance.speaker for utterance in getattr(transcript, 'utterances', []) if utterance.speaker])),
                "chapters_found": len(chapters),
                "highlights_found": len(highlights.results) if highlights and hasattr(highlights, 'results') else 0,
                "entities_found": len(entities),
                "sentiment_breakdown": {
                    "overall": overall_sentiment,
                    "positive_segments": pos_count,
                    "negative_segments": neg_count,
                    "neutral_segments": neu_count
                }
            }
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if chapters:
            response_data["chapters"] = [
                {
                    "headline": getattr(ch, 'headline', ''),
                    "start_time": f"{getattr(ch, 'start', 0)/1000/60:.1f}min",
                    "end_time": f"{getattr(ch, 'end', 0)/1000/60:.1f}min",
                    "summary": getattr(ch, 'summary', '')
                }
                for ch in chapters[:15]
            ]
        
        if highlights and hasattr(highlights, 'results'):
            response_data["key_highlights"] = [
                {
                    "text": getattr(h, 'text', ''),
                    "rank": getattr(h, 'rank', 0),
                    "start_time": f"{getattr(h, 'start', 0)/1000/60:.1f}min"
                }
                for h in highlights.results[:20]
            ]
        
        if entities_by_type:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–Ω–æ—Å—Ç–µ–π –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
            limited_entities = {}
            for entity_type, entity_list in entities_by_type.items():
                limited_entities[entity_type] = entity_list[:10]
            response_data["entities_by_type"] = limited_entities
        
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –æ—Ç AssemblyAI
        builtin_summary = getattr(transcript, 'summary', None)
        if builtin_summary:
            response_data["assemblyai_summary"] = builtin_summary

        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        return jsonify({"error": f"–û—à–∏–±–∫–∞: {str(e)[:300]}"}), 500
    finally:
        if input_path and os.path.exists(input_path):
            os.remove(input_path)

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    logger.info(f"‚úÖ AssemblyAI –±–µ–∑ ffmpeg —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É: {port}")
    app.run(host="0.0.0.0", port=port, debug=False, threaded=True)
