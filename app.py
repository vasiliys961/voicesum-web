# app.py - –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º AssemblyAI
from flask import Flask, render_template, request, jsonify
import os
import tempfile
import assemblyai as aai
from openai import OpenAI
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder="static", template_folder="templates")
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500 MB

# === API –∫–ª—é—á–∏ ===
ASSEMBLYAI_API_KEY = os.environ.get("ASSEMBLYAI_API_KEY", "fb277d535ab94838bc14cc2f687b30be")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
TEMP_DIR = tempfile.mkdtemp(prefix="voicesum_hybrid_")

logger.info(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞: {TEMP_DIR}")

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ AssemblyAI ===
aai.settings.api_key = ASSEMBLYAI_API_KEY
logger.info("üîë AssemblyAI API –∫–ª—é—á –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ ===
def get_transcription_config_auto():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º —è–∑—ã–∫–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
    return aai.TranscriptionConfig(
        # === –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
        speech_model=aai.SpeechModel.best,  # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        # language_code –ù–ï —É–∫–∞–∑—ã–≤–∞–µ–º - –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ
        
        # === AI —Ñ—É–Ω–∫—Ü–∏–∏ (–≤—Å–µ –≤–∫–ª—é—á–µ–Ω—ã –¥–ª—è –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è) ===
        speaker_labels=True,  # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤
        speakers_expected=2,  # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        auto_highlights=True,  # –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
        auto_chapters=True,   # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥–ª–∞–≤—ã
        
        sentiment_analysis=True,  # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π
        entity_detection=True,    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        
        content_safety=True,      # –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        iab_categories=True,      # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Ç–µ–º–∞–º
        
        summarization=True,       # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ
        summary_model=aai.SummarizationModel.informative,
        summary_type=aai.SummarizationType.bullets,
        
        # === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ===
        punctuate=True,          # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        format_text=True,        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        dual_channel=False,      # –ú–æ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
        
        # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        disfluencies=False,      # –£–±–∏—Ä–∞–µ–º "—ç–º", "–∞—Ö"
        filter_profanity=False,  # –ù–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –º–∞—Ç
        redact_pii=False,        # –ù–µ —Å–∫—Ä—ã–≤–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    )

def get_transcription_config_russian():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    return aai.TranscriptionConfig(
        # === –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
        speech_model=aai.SpeechModel.best,  # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        language_code="ru",  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
        
        # === –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        speaker_labels=True,  # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤ ‚úÖ
        speakers_expected=2,  # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        entity_detection=True,    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π ‚úÖ
        
        # === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ===
        punctuate=True,          # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        format_text=True,        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        dual_channel=False,      # –ú–æ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
        
        # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
        disfluencies=False,      # –£–±–∏—Ä–∞–µ–º "—ç–º", "–∞—Ö"
        filter_profanity=False,  # –ù–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –º–∞—Ç
        redact_pii=False,        # –ù–µ —Å–∫—Ä—ã–≤–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    )

def transcribe_with_fallback(file_path):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å fallback: —Å–Ω–∞—á–∞–ª–∞ –∞–≤—Ç–æ, –ø–æ—Ç–æ–º —Ä—É—Å—Å–∫–∏–π"""
    
    # –ü—Ä–æ–±—É–µ–º —Å –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º (–≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏)
    try:
        logger.info("üåç –ü—Ä–æ–±—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º —è–∑—ã–∫–∞ (–≤—Å–µ AI —Ñ—É–Ω–∫—Ü–∏–∏)...")
        config_auto = get_transcription_config_auto()
        transcriber = aai.Transcriber(config=config_auto)
        transcript = transcriber.transcribe(file_path)
        
        if transcript.status == aai.TranscriptStatus.error:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {transcript.error}")
        
        logger.info("‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∞–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º —É—Å–ø–µ—à–Ω–∞!")
        return transcript, "auto_detection_full_features"
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ: {e}")
        
        # Fallback: —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏)
        try:
            logger.info("üá∑üá∫ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏)...")
            config_ru = get_transcription_config_russian()
            transcriber = aai.Transcriber(config=config_ru)
            transcript = transcriber.transcribe(file_path)
            
            if transcript.status == aai.TranscriptStatus.error:
                raise RuntimeError(f"–û—à–∏–±–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ: {transcript.error}")
            
            logger.info("‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —É—Å–ø–µ—à–Ω–∞!")
            return transcript, "russian_limited_features"
            
        except Exception as e2:
            logger.error(f"‚ùå –û–±–∞ –º–µ—Ç–æ–¥–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏: {e2}")
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å: –∞–≤—Ç–æ({e}), —Ä—É—Å—Å–∫–∏–π({e2})")

# === –£–º–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ ===
class AdvancedSummarizer:
    def __init__(self, openrouter_key):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=openrouter_key
        ) if openrouter_key else None
    
    def create_smart_summary(self, transcript_result, transcription_method):
        """–°–æ–∑–¥–∞–µ—Ç —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö AssemblyAI"""
        if not self.client:
            return self._create_basic_summary(transcript_result, transcription_method)
        
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            transcript = transcript_result.text
            chapters = getattr(transcript_result, 'chapters', None) or []
            highlights = getattr(transcript_result, 'auto_highlights', None)
            sentiment = getattr(transcript_result, 'sentiment_analysis_results', None) or []
            entities = getattr(transcript_result, 'entities', None) or []
            builtin_summary = getattr(transcript_result, 'summary', None)
            detected_language = getattr(transcript_result, 'language_code', 'unknown')
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –±–æ–≥–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = f"–ü–û–õ–ù–´–ô –¢–†–ê–ù–°–ö–†–ò–ü–¢:\n{transcript[:25000]}\n\n"
            context += f"–ú–ï–¢–û–î –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–ò: {transcription_method}\n"
            context += f"–û–ü–†–ï–î–ï–õ–ï–ù–ù–´–ô –Ø–ó–´–ö: {detected_language}\n\n"
            
            if chapters:
                context += "üìö –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ì–õ–ê–í–´:\n"
                for i, chapter in enumerate(chapters[:10], 1):
                    headline = getattr(chapter, 'headline', f'–ì–ª–∞–≤–∞ {i}')
                    start_time = getattr(chapter, 'start', 0) / 1000 / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö
                    context += f"{i}. {headline} ({start_time:.1f}–º–∏–Ω)\n"
                context += "\n"
            
            if highlights and hasattr(highlights, 'results'):
                context += "üí° –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:\n"
                for highlight in highlights.results[:15]:
                    text = getattr(highlight, 'text', '')
                    rank = getattr(highlight, 'rank', 0)
                    if text:
                        context += f"‚Ä¢ {text} (–≤–∞–∂–Ω–æ—Å—Ç—å: {rank:.2f})\n"
                context += "\n"
            
            if entities:
                context += "üè∑Ô∏è –£–ü–û–ú–Ø–ù–£–¢–´–ï –°–£–©–ù–û–°–¢–ò:\n"
                entity_groups = {}
                for entity in entities[:25]:
                    entity_type = getattr(entity, 'entity_type', 'other')
                    entity_text = getattr(entity, 'text', '')
                    if entity_type not in entity_groups:
                        entity_groups[entity_type] = []
                    if entity_text not in entity_groups[entity_type]:
                        entity_groups[entity_type].append(entity_text)
                
                for entity_type, texts in entity_groups.items():
                    context += f"  {entity_type}: {', '.join(texts[:5])}\n"
                context += "\n"
            
            if builtin_summary:
                context += f"ü§ñ –ë–ê–ó–û–í–û–ï –†–ï–ó–Æ–ú–ï ASSEMBLYAI:\n{builtin_summary}\n\n"
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            if sentiment:
                positive_count = sum(1 for s in sentiment if getattr(s, 'sentiment', '') == 'POSITIVE')
                negative_count = sum(1 for s in sentiment if getattr(s, 'sentiment', '') == 'NEGATIVE')
                total_sentiment = len(sentiment)
                if total_sentiment > 0:
                    context += f"üé≠ –û–ë–©–ê–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨: {positive_count}/{total_sentiment} –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö, {negative_count}/{total_sentiment} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö\n\n"
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–∞
            content_language = detected_language if detected_language != 'unknown' else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'
            
            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            if transcription_method == "auto_detection_full_features":
                system_prompt = (
                    f"–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∞—É–¥–∏–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –ø–æ–ª–Ω–æ–º—É –Ω–∞–±–æ—Ä—É AI-–∞–Ω–∞–ª–∏–∑–∞. "
                    f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω –Ω–∞ —è–∑—ã–∫–µ: {content_language}. "
                    f"–ò—Å–ø–æ–ª—å–∑—É–π –≤—Å–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –≥–ª–∞–≤—ã, –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã, —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å, —Å—É—â–Ω–æ—Å—Ç–∏. "
                    f"–í–ê–ñ–ù–û: –°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –°–¢–†–û–ì–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï, "
                    f"–¥–∞–∂–µ –µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ. –ü–µ—Ä–µ–≤–µ–¥–∏ –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –∏ –≤—ã–≤–æ–¥—ã."
                )
            else:
                system_prompt = (
                    f"–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∞—É–¥–∏–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
                    f"–£ —Ç–µ–±—è –µ—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –∏ –±–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—É—â–Ω–æ—Å—Ç—è—Ö –∏ —Å–ø–∏–∫–µ—Ä–∞—Ö. "
                    f"–°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï, "
                    f"–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ—Å—Ç—É–ø–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
                )
            
            system_prompt += f"""

–í–ê–ñ–ù–´–ï –£–ö–ê–ó–ê–ù–ò–Ø:
- –†–ï–ó–Æ–ú–ï –î–û–õ–ñ–ù–û –ë–´–¢–¨ –°–¢–†–û–ì–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
- –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ {content_language}, –ø–µ—Ä–µ–≤–µ–¥–∏ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
- –ò–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–ª—è—Ç—å –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ, –Ω–æ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –ø–µ—Ä–µ–≤–æ–¥–∏ –∏–ª–∏ –ø–æ—è—Å–Ω—è–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º

–°–¢–†–£–ö–¢–£–†–ê –†–ï–ó–Æ–ú–ï:
üìã –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - —Å—É—Ç—å –∑–∞–ø–∏—Å–∏)
üåç –Ø–ó–´–ö –ö–û–ù–¢–ï–ù–¢–ê: {content_language}
üéØ –û–°–ù–û–í–ù–´–ï –¢–ï–ú–´ –ò –†–ê–ó–î–ï–õ–´ (—Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –µ—Å–ª–∏ –µ—Å—Ç—å)
üë• –£–ß–ê–°–¢–ù–ò–ö–ò –ò –†–û–õ–ò (–µ—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Å–ø–∏–∫–µ—Ä—ã)
üí° –ö–õ–Æ–ß–ï–í–´–ï –ò–ù–°–ê–ô–¢–´ –ò –í–´–í–û–î–´
üìä –í–ê–ñ–ù–´–ï –§–ê–ö–¢–´, –¶–ò–§–†–´, –î–ê–¢–´
üé≠ –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
‚úÖ –†–ï–®–ï–ù–ò–Ø, –î–ï–ô–°–¢–í–ò–Ø, NEXT STEPS
üè∑Ô∏è –ö–õ–Æ–ß–ï–í–´–ï –ü–ï–†–°–û–ù–´ –ò –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –í–°–ï –†–ï–ó–Æ–ú–ï –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
- –í—ã–¥–µ–ª–∏ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ
- –°–æ—Ö—Ä–∞–Ω—è–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω
- –ü–µ—Ä–µ–≤–æ–¥–∏ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –ø–æ–Ω—è—Ç–∏—è
- –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –ò—Å–ø–æ–ª—å–∑—É–π –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç AssemblyAI
"""
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ
            response = self.client.chat.completions.create(
                model="anthropic/claude-3-haiku",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": context}
                ],
                max_tokens=1500,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–º–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
            return self._create_basic_summary(transcript_result, transcription_method)
    
    def _create_basic_summary(self, transcript_result, transcription_method):
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –∏–∑ –¥–∞–Ω–Ω—ã—Ö AssemblyAI –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï"""
        summary_parts = []
        detected_language = getattr(transcript_result, 'language_code', 'unknown')
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        language_names = {
            'en': '–∞–Ω–≥–ª–∏–π—Å–∫–∏–π',
            'ru': '—Ä—É—Å—Å–∫–∏–π', 
            'es': '–∏—Å–ø–∞–Ω—Å–∫–∏–π',
            'fr': '—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π',
            'de': '–Ω–µ–º–µ—Ü–∫–∏–π',
            'it': '–∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π',
            'pt': '–ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π',
            'zh': '–∫–∏—Ç–∞–π—Å–∫–∏–π',
            'ja': '—è–ø–æ–Ω—Å–∫–∏–π',
            'ko': '–∫–æ—Ä–µ–π—Å–∫–∏–π',
            'unknown': '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'
        }
        
        language_display = language_names.get(detected_language, detected_language)
        
        summary_parts.append(f"üîß –ú–ï–¢–û–î –û–ë–†–ê–ë–û–¢–ö–ò: {transcription_method}")
        summary_parts.append(f"üåç –Ø–ó–´–ö –ö–û–ù–¢–ï–ù–¢–ê: {language_display}")
        
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ (–ø–µ—Ä–µ–≤–æ–¥–∏–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        builtin_summary = getattr(transcript_result, 'summary', None)
        if builtin_summary:
            if detected_language != 'ru' and detected_language != 'unknown':
                summary_parts.append(f"\nüìã –ë–ê–ó–û–í–û–ï –†–ï–ó–Æ–ú–ï (–∞–≤—Ç–æ–ø–µ—Ä–µ–≤–æ–¥ —Å {language_display}):")
                summary_parts.append(f"[–û–†–ò–ì–ò–ù–ê–õ] {builtin_summary}")
                summary_parts.append("üìù –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —Å–º. —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ –≤—ã—à–µ")
            else:
                summary_parts.append(f"\nüìã –ë–ê–ó–û–í–û–ï –†–ï–ó–Æ–ú–ï:\n{builtin_summary}")
        
        # –ì–ª–∞–≤—ã (–ø–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        chapters = getattr(transcript_result, 'chapters', None)
        if chapters:
            summary_parts.append("\nüéØ –û–°–ù–û–í–ù–´–ï –†–ê–ó–î–ï–õ–´:")
            for i, chapter in enumerate(chapters[:8], 1):
                headline = getattr(chapter, 'headline', f'–†–∞–∑–¥–µ–ª {i}')
                start_time = getattr(chapter, 'start', 0) / 1000 / 60
                
                if detected_language != 'ru' and detected_language != 'unknown':
                    summary_parts.append(f"{i}. [{headline}] ({start_time:.1f}–º–∏–Ω)")
                else:
                    summary_parts.append(f"{i}. {headline} ({start_time:.1f}–º–∏–Ω)")
        
        # –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å –ø–æ–º–µ—Ç–∫–æ–π –æ —è–∑—ã–∫–µ)
        highlights = getattr(transcript_result, 'auto_highlights', None)
        if highlights and hasattr(highlights, 'results'):
            summary_parts.append(f"\nüí° –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´ (–Ω–∞ {language_display}):")
            for highlight in highlights.results[:10]:
                text = getattr(highlight, 'text', '').strip()
                if text:
                    summary_parts.append(f"‚Ä¢ {text}")
        
        # –°—É—â–Ω–æ—Å—Ç–∏ (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤)
        entities = getattr(transcript_result, 'entities', None)
        if entities:
            summary_parts.append("\nüè∑Ô∏è –£–ü–û–ú–Ø–ù–£–¢–´–ï –°–£–©–ù–û–°–¢–ò:")
            entity_groups = {}
            
            # –ü–µ—Ä–µ–≤–æ–¥ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entity_type_translation = {
                'person': '–õ—é–¥–∏',
                'organization': '–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏', 
                'location': '–ú–µ—Å—Ç–∞',
                'date': '–î–∞—Ç—ã',
                'money': '–î–µ–Ω—å–≥–∏',
                'phone_number': '–¢–µ–ª–µ—Ñ–æ–Ω—ã',
                'email': 'Email',
                'product': '–ü—Ä–æ–¥—É–∫—Ç—ã',
                'event': '–°–æ–±—ã—Ç–∏—è',
                'other': '–ü—Ä–æ—á–µ–µ'
            }
            
            for entity in entities[:15]:
                entity_type = getattr(entity, 'entity_type', 'other')
                entity_text = getattr(entity, 'text', '')
                translated_type = entity_type_translation.get(entity_type, entity_type)
                
                if translated_type not in entity_groups:
                    entity_groups[translated_type] = []
                if entity_text not in entity_groups[translated_type]:
                    entity_groups[translated_type].append(entity_text)
            
            for entity_type, texts in entity_groups.items():
                if texts:
                    summary_parts.append(f"  {entity_type}: {', '.join(texts[:3])}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ –æ —è–∑—ã–∫–µ
        if detected_language != 'ru' and detected_language != 'unknown':
            summary_parts.append(f"\nüìù –ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ {language_display}. –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ –≤—ã—à–µ.")
        
        if not summary_parts:
            summary_parts.append("üìã –ë–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
        
        return "\n".join(summary_parts)

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
summarizer = AdvancedSummarizer(OPENROUTER_API_KEY)

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
def analyze_sentiment_overall(sentiment_results):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—â—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"""
    if not sentiment_results:
        return "not_analyzed", 0, 0, 0
    
    positive_count = sum(1 for s in sentiment_results if getattr(s, 'sentiment', '') == 'POSITIVE')
    negative_count = sum(1 for s in sentiment_results if getattr(s, 'sentiment', '') == 'NEGATIVE')
    neutral_count = len(sentiment_results) - positive_count - negative_count
    
    if positive_count > negative_count and positive_count > neutral_count:
        return "positive", positive_count, negative_count, neutral_count
    elif negative_count > positive_count and negative_count > neutral_count:
        return "negative", positive_count, negative_count, neutral_count
    else:
        return "neutral", positive_count, negative_count, neutral_count

def format_entities_by_type(entities):
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º"""
    if not entities:
        return {}
    
    entity_groups = {}
    for entity in entities:
        entity_type = getattr(entity, 'entity_type', 'other')
        entity_text = getattr(entity, 'text', '')
        
        if entity_type not in entity_groups:
            entity_groups[entity_type] = []
        
        if entity_text and entity_text not in entity_groups[entity_type]:
            entity_groups[entity_type].append(entity_text)
    
    return entity_groups

def get_transcription_features(transcription_method):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞"""
    if transcription_method == "auto_detection_full_features":
        return [
            "üéôÔ∏è –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏",
            "üåç –ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —è–∑—ã–∫–∞",
            "üë• –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤", 
            "üìö –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥–ª–∞–≤—ã",
            "üí° –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã",
            "üé≠ –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
            "üè∑Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π",
            "ü§ñ –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ",
            "üß† –£–º–Ω–æ–µ —Ä–µ–∑—é–º–µ Claude",
            "üõ°Ô∏è –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º"
        ]
    else:
        return [
            "üéôÔ∏è –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏",
            "üá∑üá∫ –†—É—Å—Å–∫–∏–π —è–∑—ã–∫",
            "üë• –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤",
            "üè∑Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π",
            "üß† –£–º–Ω–æ–µ —Ä–µ–∑—é–º–µ Claude"
        ]

# === –ú–∞—Ä—à—Ä—É—Ç—ã ===

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/health")
def health():
    return jsonify({
        "status": "ok",
        "service": "AssemblyAI Hybrid Approach",
        "api_configured": bool(ASSEMBLYAI_API_KEY),
        "openrouter_configured": bool(OPENROUTER_API_KEY),
        "transcription_methods": [
            "1. Auto-detection (full features)",
            "2. Russian language (fallback)"
        ],
        "auto_features": get_transcription_features("auto_detection_full_features"),
        "russian_features": get_transcription_features("russian_limited_features"),
        "limits": {
            "free_credits": "$50 (185 hours)",
            "max_file_size": "500MB",
            "max_duration": "unlimited",
            "parallel_processing": "5 files"
        }
    })

@app.route("/transcribe", methods=["POST"])
def transcribe():
    start_time = time.time()
    input_path = None
    
    try:
        if 'audio' not in request.files:
            return jsonify({"error": "–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω"}), 400

        file = request.files['audio']
        if file.filename == '':
            return jsonify({"error": "–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω"}), 400

        timestamp = int(time.time() * 1000)
        input_path = os.path.join(TEMP_DIR, f"hybrid_{timestamp}.{file.filename.split('.')[-1]}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        file.save(input_path)
        file_size = os.path.getsize(input_path)
        logger.info(f"üì• –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {file_size / 1024 / 1024:.1f} MB")
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        estimated_duration = file_size / 1024 / 1024  # –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ –≤ –º–∏–Ω—É—Ç–∞—Ö
        logger.info(f"üìä –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: ~{estimated_duration:.1f} –º–∏–Ω—É—Ç")
        
        # –ì–∏–±—Ä–∏–¥–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å fallback
        transcript, transcription_method = transcribe_with_fallback(input_path)
        
        if not transcript.text:
            return jsonify({"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é"}), 400
        
        logger.info(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–º: {transcription_method}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ—á–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ AssemblyAI
        audio_duration_ms = getattr(transcript, 'audio_duration', None)
        actual_duration = audio_duration_ms / 1000 / 60 if audio_duration_ms else estimated_duration
        detected_language = getattr(transcript, 'language_code', 'unknown')
        
        # –°–æ–∑–¥–∞–µ–º —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ
        logger.info("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É—é —É–º–Ω–æ–µ —Ä–µ–∑—é–º–µ...")
        summary = summarizer.create_smart_summary(transcript, transcription_method)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞
        sentiment_analysis = getattr(transcript, 'sentiment_analysis_results', None) or []
        overall_sentiment, pos_count, neg_count, neu_count = analyze_sentiment_overall(sentiment_analysis)
        
        entities = getattr(transcript, 'entities', None) or []
        entities_by_type = format_entities_by_type(entities)
        
        chapters = getattr(transcript, 'chapters', None) or []
        highlights = getattr(transcript, 'auto_highlights', None)
        
        # –ü–æ–¥—Å—á–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫—Ä–µ–¥–∏—Ç–æ–≤
        credits_used = actual_duration / 60 * 0.37  # –ø—Ä–∏–º–µ—Ä–Ω–æ $0.37 –∑–∞ —á–∞—Å
        
        total_time = time.time() - start_time
        logger.info(f"‚úÖ –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.1f}—Å")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        response_data = {
            "transcript": transcript.text,
            "summary": summary,
            "service_used": "AssemblyAI Hybrid Approach",
            "transcription_method": transcription_method,
            "detected_language": detected_language,
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            "statistics": {
                "processing_time": f"{total_time:.1f}s",
                "audio_duration": f"{actual_duration:.1f}min", 
                "file_size": f"{file_size / 1024 / 1024:.1f}MB",
                "confidence": getattr(transcript, 'confidence', 0),
                "credits_used": f"${credits_used:.3f}",
                "words_count": len(transcript.text.split()) if transcript.text else 0
            },
            
            # AI –∞–Ω–∞–ª–∏–∑
            "ai_analysis": {
                "method_used": transcription_method,
                "speakers_detected": len(set([utterance.speaker for utterance in getattr(transcript, 'utterances', []) if utterance.speaker])),
                "chapters_found": len(chapters),
                "highlights_found": len(highlights.results) if highlights and hasattr(highlights, 'results') else 0,
                "entities_found": len(entities),
                "sentiment_breakdown": {
                    "overall": overall_sentiment,
                    "positive_segments": pos_count,
                    "negative_segments": neg_count,
                    "neutral_segments": neu_count
                },
                "features_available": get_transcription_features(transcription_method)
            }
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if chapters:
            response_data["chapters"] = [
                {
                    "headline": getattr(ch, 'headline', ''),
                    "start_time": f"{getattr(ch, 'start', 0)/1000/60:.1f}min",
                    "end_time": f"{getattr(ch, 'end', 0)/1000/60:.1f}min",
                    "summary": getattr(ch, 'summary', '')
                }
                for ch in chapters[:15]
            ]
        
        if highlights and hasattr(highlights, 'results'):
            response_data["key_highlights"] = [
                {
                    "text": getattr(h, 'text', ''),
                    "rank": getattr(h, 'rank', 0),
                    "start_time": f"{getattr(h, 'start', 0)/1000/60:.1f}min"
                }
                for h in highlights.results[:20]
            ]
        
        if entities_by_type:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–Ω–æ—Å—Ç–µ–π –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
            limited_entities = {}
            for entity_type, entity_list in entities_by_type.items():
                limited_entities[entity_type] = entity_list[:10]
            response_data["entities_by_type"] = limited_entities
        
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –æ—Ç AssemblyAI (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        builtin_summary = getattr(transcript, 'summary', None)
        if builtin_summary:
            response_data["assemblyai_summary"] = builtin_summary
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–µ—Ç–æ–¥–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        if transcription_method == "auto_detection_full_features":
            response_data["method_info"] = {
                "name": "–ê–≤—Ç–æ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —è–∑—ã–∫–∞",
                "features": "–í—Å–µ AI —Ñ—É–Ω–∫—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–Ω—ã",
                "language_detected": detected_language,
                "fallback_used": False
            }
        else:
            response_data["method_info"] = {
                "name": "–†—É—Å—Å–∫–∏–π —è–∑—ã–∫ (fallback)",
                "features": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–π",
                "language_forced": "ru",
                "fallback_used": True
            }

        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        return jsonify({"error": f"–û—à–∏–±–∫–∞: {str(e)[:300]}"}), 500
    finally:
        if input_path and os.path.exists(input_path):
            os.remove(input_path)

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    logger.info(f"‚úÖ AssemblyAI Hybrid —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É: {port}")
    app.run(host="0.0.0.0", port=port, debug=False, threaded=True)
